import os
import numpy as np
import open3d as o3d
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D
import plotly.graph_objects as go
import trimesh
import pyrender
from PIL import Image
import cv2
import logging
import io
import base64
from typing import List, Dict, Tuple, Optional, Union, Any, Literal, Callable

logger = logging.getLogger(__name__)

class DepthReconstructor:
    """
    Class for 3D reconstruction and visualization from depth maps generated by the ImageProcessor
    """
    
    def __init__(self):
        """Initialize the 3D reconstruction module"""
        logger.info("Initializing 3D Reconstructor")
        
        # Define all available visualization methods
        self.visualization_methods = {
            "depth_map": "Colored Depth Map (2D)",
            "pointcloud_mpl": "Matplotlib Point Cloud (3D)",
            "enhanced_3d": "Enhanced 3D Reconstruction",
            "lrm_3d": "LRM 3D Reconstruction"
        }
    
    def render_depth_map(self, depth_map: np.ndarray, colormap: int = cv2.COLORMAP_INFERNO,
                         width: int = 800, height: int = 600) -> np.ndarray:
        """
        Render a depth map using OpenCV's colormaps for direct visualization.
        This is the most reliable visualization method and serves as a fallback.
        
        Args:
            depth_map: Depth map as numpy array
            colormap: OpenCV colormap to use
            width: Desired output width
            height: Desired output height
            
        Returns:
            Rendered colorized depth map as numpy array
        """
        logger.info(f"Rendering depth map with colormap {colormap}")
        
        # Ensure depth map is valid
        if depth_map is None or depth_map.size == 0:
            logger.warning("Invalid depth map provided")
            return np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
            
        # Normalize depth map to 0-255 range
        if depth_map.max() > 0:
            normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
        else:
            logger.warning("Depth map has no valid depth values")
            return np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
        
        # Apply colormap
        colored = cv2.applyColorMap(normalized, colormap)
        
        # Resize to desired dimensions
        if colored.shape[0] != height or colored.shape[1] != width:
            colored = cv2.resize(colored, (width, height))
        
        # Convert to RGB (from BGR)
        colored_rgb = cv2.cvtColor(colored, cv2.COLOR_BGR2RGB)
        
        # Normalize to 0-1 for consistent output format
        return colored_rgb.astype(np.float32) / 255.0
        
    def render_pointcloud_matplotlib(self, depth_map: np.ndarray, image: Image.Image,
                                   width: int = 800, height: int = 600,
                                   downsample_factor: int = 4) -> np.ndarray:
        """
        Render a 3D point cloud using Matplotlib with corrected orientation.
        This can serve as a fallback when more advanced visualization fails.
        
        Args:
            depth_map: Depth map as numpy array
            image: Original color image
            width: Desired output width
            height: Desired output height
            downsample_factor: Factor by which to downsample the point cloud for rendering
            
        Returns:
            Rendered point cloud as numpy array
        """
        logger.info(f"Rendering point cloud with Matplotlib (downsample={downsample_factor})")
        
        try:
            # Create a figure with the right dimensions
            dpi = 100
            fig = plt.figure(figsize=(width/dpi, height/dpi), dpi=dpi)
            ax = fig.add_subplot(111, projection='3d')
            
            # Normalize depth map
            if depth_map.max() <= 255:
                depth_norm = depth_map.astype(np.float32) / 255.0
            else:
                depth_norm = depth_map.astype(np.float32) / 1000.0

            # Get color image as RGB numpy array
            color_img = np.array(image.convert('RGB'))
            
            # Ensure dimensions match
            if color_img.shape[:2] != depth_map.shape[:2]:
                color_img = cv2.resize(color_img, (depth_map.shape[1], depth_map.shape[0]))
            
            # Downsample for better performance
            h, w = depth_norm.shape
            y_indices, x_indices = np.mgrid[0:h:downsample_factor, 0:w:downsample_factor]
            
            # Get 3D coordinates
            z = depth_norm[y_indices, x_indices]
            x = x_indices
            y = y_indices  # Will be inverted to correct orientation
            colors = color_img[y_indices, x_indices] / 255.0
            
            # Flatten arrays for scatter plot
            x = x.flatten()
            y = y.flatten()
            z = z.flatten()
            colors = colors.reshape(-1, 3)
            
            # Filter out invalid points
            valid = (z > 0)
            x = x[valid]
            y = y[valid]
            z = z[valid]
            colors = colors[valid]
            
            # Normalize spatial coordinates
            x = (x - w/2) / w
            y = -(y - h/2) / h  # Invert Y axis for correct orientation
            
            # Create scatter plot with colors
            # Use x, z, -y for correct orientation (negative y makes the model upright)
            ax.scatter(x, z, -y, c=colors, s=3, alpha=0.8)
            
            # Set equal aspect ratio and labels
            ax.set_box_aspect([1, 1, 1])
            ax.set_xlabel('X')
            ax.set_ylabel('Z (Depth)')
            ax.set_zlabel('Y')
            ax.set_title('3D Point Cloud')
            
            # Use light gray background for better visibility
            ax.set_facecolor([0.9, 0.9, 0.9])
            fig.patch.set_facecolor([0.9, 0.9, 0.9])
            
            # Set an improved viewpoint
            ax.view_init(elev=20, azim=-35)  # Adjusted for better orientation
            
            # Remove grid for cleaner visualization
            ax.grid(False)
            
            # Normalize axis directions
            ax.set_xlim([-1, 1])
            ax.set_zlim([-1, 1])
            
            # Capture the plot as an image
            fig.tight_layout(pad=0)
            with io.BytesIO() as buf:
                fig.savefig(buf, format='png', bbox_inches='tight', facecolor=fig.get_facecolor())
                buf.seek(0)
                img = np.array(Image.open(buf))
            
            # Close the figure to free memory
            plt.close(fig)
            
            # Convert to float32 and normalize
            return img.astype(np.float32) / 255.0
            
        except Exception as e:
            logger.warning(f"Error rendering with Matplotlib: {str(e)}")
            # Fall back to a simple depth map visualization
            return self.render_depth_map(depth_map, width=width, height=height)
    
    def render_pointcloud_plotly(self, depth_map: np.ndarray, image: Image.Image,
                               width: int = 800, height: int = 600,
                               downsample_factor: int = 4) -> np.ndarray:
        """
        Render a 3D point cloud using Plotly for high-quality visualization.
        
        Args:
            depth_map: Depth map as numpy array
            image: Original color image
            width: Desired output width
            height: Desired output height
            downsample_factor: Factor by which to downsample the point cloud
            
        Returns:
            Rendered point cloud as numpy array
        """
        logger.info(f"Rendering point cloud with Plotly (downsample={downsample_factor})")
        
        try:
            # Normalize depth map
            if depth_map.max() <= 255:
                depth_norm = depth_map.astype(np.float32) / 255.0
            else:
                depth_norm = depth_map.astype(np.float32) / 1000.0

            # Get color image as RGB numpy array
            color_img = np.array(image.convert('RGB'))
            
            # Ensure dimensions match
            if color_img.shape[:2] != depth_map.shape[:2]:
                color_img = cv2.resize(color_img, (depth_map.shape[1], depth_map.shape[0]))
            
            # Downsample for better performance
            h, w = depth_norm.shape
            y_indices, x_indices = np.mgrid[0:h:downsample_factor, 0:w:downsample_factor]
            
            # Get 3D coordinates
            z = depth_norm[y_indices, x_indices]
            x = x_indices
            y = y_indices
            colors = color_img[y_indices, x_indices]
            
            # Flatten arrays for scatter plot
            x = x.flatten()
            y = y.flatten()
            z = z.flatten()
            
            # Filter out invalid points
            valid = (z > 0)
            x = x[valid]
            y = y[valid]
            z = z[valid]
            
            # Normalize spatial coordinates
            x = (x - w/2) / w
            y = -(y - h/2) / h  # Invert Y axis for correct orientation
            
            # Extract RGB values from the image for Plotly
            r = colors[..., 0].flatten()[valid]
            g = colors[..., 1].flatten()[valid]
            b = colors[..., 2].flatten()[valid]
            
            # Create color strings in 'rgb(r,g,b)' format
            color_strs = [f'rgb({r[i]},{g[i]},{b[i]})' for i in range(len(r))]
            
            # Create the 3D scatter plot
            fig = go.Figure(data=[go.Scatter3d(
                x=x,
                y=z,  # Use z for y-axis (depth)
                z=-y,  # Negative y for correct orientation
                mode='markers',
                marker=dict(
                    size=2,
                    color=color_strs,
                    opacity=0.8
                )
            )])
            
            # Set layout for better visualization
            fig.update_layout(
                width=width,
                height=height,
                scene=dict(
                    xaxis_title='X',
                    yaxis_title='Z (Depth)',
                    zaxis_title='Y',
                    aspectratio=dict(x=1, y=1, z=1),
                    camera=dict(
                        eye=dict(x=1.2, y=1.2, z=1.2),
                        up=dict(x=0, y=0, z=1)
                    ),
                    xaxis=dict(showgrid=False, zeroline=False),
                    yaxis=dict(showgrid=False, zeroline=False),
                    zaxis=dict(showgrid=False, zeroline=False)
                ),
                margin=dict(l=0, r=0, b=0, t=0),
                paper_bgcolor='rgb(240, 240, 240)',
                plot_bgcolor='rgb(240, 240, 240)'
            )
            
            # Render to image
            img_bytes = fig.to_image(format="png")
            img = np.array(Image.open(io.BytesIO(img_bytes)))
            
            # Convert to float32 and normalize
            return img.astype(np.float32) / 255.0
            
        except Exception as e:
            logger.warning(f"Error rendering with Plotly: {str(e)}")
            # Fall back to Matplotlib rendering
            return self.render_pointcloud_matplotlib(depth_map, image, width, height, downsample_factor)
    
    def is_headless_environment(self) -> bool:
        """
        Check if running in a headless environment (no display)
        
        For Trimesh, this is mainly informational as it can render in headless environments.
        
        Returns:
            True if headless, False otherwise
        """
        # Check for DISPLAY environment variable (X11)
        display = os.environ.get('DISPLAY', '')
        if not display:
            logger.info("No DISPLAY environment variable found - headless environment")
            return True
        
        # Check for Wayland display
        wayland_display = os.environ.get('WAYLAND_DISPLAY', '')
        if not wayland_display and not display:
            logger.info("No X11 or Wayland display found - headless environment")
            return True
            
        # Check for SSH connection without X forwarding
        if 'SSH_CONNECTION' in os.environ and not os.environ.get('XAUTHORITY'):
            logger.info("SSH connection without X forwarding detected")
            return True
        
        return False
    
    def render_pointcloud_trimesh(self, 
                                point_cloud: Optional[o3d.geometry.PointCloud] = None,
                                depth_map: Optional[np.ndarray] = None, 
                                image: Optional[Image.Image] = None,
                                width: int = 800, 
                                height: int = 600,
                                downsample_factor: int = 3) -> np.ndarray:
        """
        Enhanced Trimesh-based point cloud renderer with extensive options and fallbacks.
        Works well in headless environments without requiring a virtual framebuffer.
        
        Can render either a provided point cloud or generate one from depth map & image.
        
        Args:
            point_cloud: Optional Open3D point cloud to render (takes precedence if provided)
            depth_map: Depth map as numpy array (used if point_cloud not provided)
            image: Original color image (used if point_cloud not provided)
            width: Desired output width
            height: Desired output height
            downsample_factor: Factor by which to downsample the point cloud
            
        Returns:
            Rendered point cloud as numpy array
        """
        logger.info(f"Rendering point cloud with Trimesh (downsample={downsample_factor})")
        
        try:
            # Process differently based on whether we got a point cloud or depth/image
            if point_cloud is not None and len(point_cloud.points) > 0:
                # Use the provided point cloud
                logger.info(f"Rendering provided point cloud with {len(point_cloud.points)} points")
                
                # Convert Open3D point cloud to numpy arrays
                points = np.asarray(point_cloud.points)
                colors = np.asarray(point_cloud.colors)
                
                # Create Trimesh point cloud
                cloud = trimesh.PointCloud(vertices=points)
                
                # Add colors if available
                if colors.shape[0] == points.shape[0]:
                    # Convert from float [0,1] to uint8 [0,255] for Trimesh
                    cloud.colors = (colors * 255).astype(np.uint8)
                
            else:
                # Generate point cloud from depth map and image
                if depth_map is None or image is None:
                    raise ValueError("Either point_cloud or both depth_map and image must be provided")
                
                # Normalize depth map
                if depth_map.max() <= 255:
                    depth_norm = depth_map.astype(np.float32) / 255.0
                else:
                    depth_norm = depth_map.astype(np.float32) / 1000.0
    
                # Get color image as RGB numpy array
                color_img = np.array(image.convert('RGB'))
                
                # Ensure dimensions match
                if color_img.shape[:2] != depth_map.shape[:2]:
                    color_img = cv2.resize(color_img, (depth_map.shape[1], depth_map.shape[0]))
                
                # Downsample for better performance
                h, w = depth_norm.shape
                y_indices, x_indices = np.mgrid[0:h:downsample_factor, 0:w:downsample_factor]
                
                # Get 3D coordinates
                z = depth_norm[y_indices, x_indices]
                x = x_indices
                y = y_indices
                colors = color_img[y_indices, x_indices]
                
                # Flatten arrays
                x = x.flatten()
                y = y.flatten()
                z = z.flatten()
                rgb = colors.reshape(-1, 3)
                
                # Filter out invalid points
                valid = (z > 0)
                x = x[valid]
                y = y[valid]
                z = z[valid]
                rgb = rgb[valid]
                
                # Normalize spatial coordinates
                x = (x - w/2) / w
                y = -(y - h/2) / h  # Invert Y axis
                
                # Create point cloud data array with proper orientation
                points = np.column_stack((x, z, -y))  # Use -y for correct orientation
                
                # Create Trimesh point cloud
                cloud = trimesh.PointCloud(vertices=points, colors=rgb)
            
            # Create a scene with the point cloud
            scene = trimesh.Scene()
            scene.add_geometry(cloud)
            
            # Get the extents of the scene for camera placement
            bounds = scene.bounds
            extents = bounds[1] - bounds[0]
            
            # Set up a camera looking at the center of the point cloud
            center = scene.centroid
            
            # Calculate camera position: looking from the front, slightly above
            camera_position = center + np.array([0, -2, 0.5]) * max(extents)
            
            # Look at the center of the cloud
            camera_target = center
            
            # Define the "up" direction
            camera_up = np.array([0, 0, 1])
            
            # Compute the camera transform
            camera_transform = trimesh.transformations.look_at(
                camera_position,
                camera_target,
                camera_up
            )
            
            # Create a scene camera
            camera = trimesh.scene.Camera(
                resolution=(width, height),
                fov=(60, 40),  # horizontal, vertical field of view in degrees
                transform=camera_transform
            )
            
            # Add camera to the scene
            scene.camera = camera
            
            # Render the scene
            # Use pyrender for offscreen rendering
            rendered = scene.save_image(
                resolution=(width, height),
                visible=True,
                background=[230, 230, 230, 255]  # Light gray background
            )
            
            # Convert the rendered image to a numpy array
            img = np.asarray(rendered).astype(np.float32) / 255.0
            
            # Check if the result is valid (not all white or black)
            if img.mean() < 0.05 or img.mean() > 0.95:
                logger.warning(f"Trimesh rendering produced invalid image (too bright/dark): {img.mean()}")
                raise ValueError("Invalid rendering output")
            
            return img
            
        except Exception as e:
            logger.warning(f"Error rendering with Trimesh: {str(e)}")
            
            # Try Plotly next (better than Matplotlib for 3D)
            try:
                logger.info("Falling back to Plotly 3D renderer")
                if point_cloud is not None:
                    # For point cloud input, convert to Open3D rendering first
                    # then use direct rendering for the point cloud
                    return self.render_pointcloud_image(point_cloud, width, height, zoom=0.8)
                else:
                    return self.render_pointcloud_plotly(depth_map, image, width, height, downsample_factor)
            except Exception as e2:
                logger.warning(f"Error with Plotly fallback: {str(e2)}")
                
                # Fall back to Matplotlib rendering as last resort
                logger.info("Falling back to Matplotlib renderer (final fallback)")
                if point_cloud is not None:
                    # We need to first get a depth map for Matplotlib
                    depths = np.asarray([p[2] for p in point_cloud.points])
                    if len(depths) > 0:
                        min_depth = min(depths)
                        max_depth = max(depths)
                        # Create a simple depth map
                        synthetic_depth = np.ones((height, width)) * min_depth
                        # Just render a colored depth map as last resort
                        return self.render_depth_map(
                            np.asarray(point_cloud.points)[:, 2].reshape(-1, 1), 
                            width=width, 
                            height=height
                        )
                    else:
                        return np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
                else:
                    return self.render_pointcloud_matplotlib(depth_map, image, width, height, downsample_factor)
    
    def depth_to_pointcloud(self, 
                           depth_map: np.ndarray, 
                           image: Image.Image,
                           focal_length: float = 525.0, 
                           scale_factor: float = 1000.0,
                           downsample_factor: int = 2) -> o3d.geometry.PointCloud:
        """
        Convert a depth map to a 3D point cloud
        
        Args:
            depth_map: Depth map as numpy array
            image: Original color image corresponding to the depth map
            focal_length: Camera focal length (approximation)
            scale_factor: Depth scale factor
            downsample_factor: Factor by which to downsample the point cloud
            
        Returns:
            Open3D PointCloud object
        """
        logger.info(f"Converting depth map to point cloud (downsample={downsample_factor})")
        
        # Ensure depth_map is properly scaled from 0-255 to actual depth values
        if depth_map.max() <= 255:
            depth_norm = depth_map.astype(np.float32) / 255.0
        else:
            depth_norm = depth_map.astype(np.float32) / scale_factor
            
        # Get color image as RGB numpy array
        color_img = np.array(image.convert('RGB'))
        
        # Ensure dimensions match
        if color_img.shape[:2] != depth_map.shape[:2]:
            color_img = cv2.resize(color_img, (depth_map.shape[1], depth_map.shape[0]))
        
        # Downsample for better performance
        if downsample_factor > 1:
            depth_norm = depth_norm[::downsample_factor, ::downsample_factor]
            color_img = color_img[::downsample_factor, ::downsample_factor]
            
        # Get image dimensions
        height, width = depth_norm.shape
        
        # Create meshgrid of coordinates
        y, x = np.mgrid[0:height, 0:width]
        
        # Compute 3D coordinates
        # Center the coordinate system at the image center
        cx, cy = width / 2, height / 2
        x = (x - cx) * depth_norm / focal_length
        y = (y - cy) * depth_norm / focal_length
        z = depth_norm
        
        # Flatten and combine into points
        points = np.stack((x.flatten(), y.flatten(), z.flatten()), axis=1)
        colors = color_img.reshape(-1, 3) / 255.0
        
        # Remove invalid points (zero depth)
        valid_indices = z.flatten() > 0
        points = points[valid_indices]
        colors = colors[valid_indices]
        
        # Create Open3D point cloud
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        pcd.colors = o3d.utility.Vector3dVector(colors)
        
        # Optionally filter noise and outliers
        pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
        
        return pcd
    
    def pointcloud_to_mesh(self, pcd: o3d.geometry.PointCloud, depth_threshold: float = 0.05) -> o3d.geometry.TriangleMesh:
        """
        Convert point cloud to mesh
        
        Args:
            pcd: Open3D PointCloud
            depth_threshold: Threshold for depth difference between connected points
            
        Returns:
            Open3D TriangleMesh
        """
        logger.info("Converting point cloud to mesh")
        
        # Estimate normals if they don't exist
        if not pcd.has_normals():
            pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))
            pcd.orient_normals_towards_camera_location()
        
        # Create mesh using Poisson surface reconstruction
        mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=8)
        
        # Remove low density vertices
        vertices_to_remove = densities < np.quantile(densities, 0.1)
        mesh.remove_vertices_by_mask(vertices_to_remove)
        
        return mesh
    
    def save_pointcloud(self, pcd: o3d.geometry.PointCloud, filename: str) -> str:
        """
        Save point cloud to file
        
        Args:
            pcd: Open3D PointCloud
            filename: Name for the output file (without extension)
            
        Returns:
            Path to the saved file
        """
        # Save as PLY
        output_path = f"{filename}.ply"
        o3d.io.write_point_cloud(output_path, pcd)
        logger.info(f"Point cloud saved to {output_path}")
        return output_path
    
    def save_mesh(self, mesh: o3d.geometry.TriangleMesh, filename: str) -> str:
        """
        Save mesh to file
        
        Args:
            mesh: Open3D TriangleMesh
            filename: Name for the output file (without extension)
            
        Returns:
            Path to the saved file
        """
        # Save as OBJ
        output_path = f"{filename}.obj"
        o3d.io.write_triangle_mesh(output_path, mesh)
        logger.info(f"Mesh saved to {output_path}")
        return output_path
    
    def visualize_pointcloud(self, pcd: o3d.geometry.PointCloud) -> None:
        """
        Visualize point cloud using Open3D visualizer
        
        Args:
            pcd: Open3D PointCloud
        """
        o3d.visualization.draw_geometries([pcd])
    
    def visualize_mesh(self, mesh: o3d.geometry.TriangleMesh) -> None:
        """
        Visualize mesh using Open3D visualizer
        
        Args:
            mesh: Open3D TriangleMesh
        """
        o3d.visualization.draw_geometries([mesh])
    
    def render_pointcloud_image(self, pcd: o3d.geometry.PointCloud, 
                               width: int = 800, height: int = 600,
                               zoom: float = 0.8) -> np.ndarray:
        """
        Render point cloud to image without opening a window
        
        Args:
            pcd: Open3D PointCloud
            width: Image width
            height: Image height
            zoom: Camera zoom factor
            
        Returns:
            Rendered image as numpy array
        """
        # Debug information
        logger.info(f"Rendering point cloud with {len(pcd.points)} points")
        
        # Check if point cloud has points
        if len(pcd.points) == 0:
            logger.warning("Point cloud is empty, creating a message image instead")
            fallback_img = np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
            return fallback_img
            
        vis = o3d.visualization.Visualizer()
        vis.create_window(visible=False, width=width, height=height)
        vis.add_geometry(pcd)
        
        # Configure camera view
        try:
            view_control = vis.get_view_control()
            if view_control is not None:
                # Position the camera to see the entire point cloud
                camera_params = view_control.convert_to_pinhole_camera_parameters()
                
                # Get the bounding box of the point cloud
                bbox = pcd.get_axis_aligned_bounding_box()
                center = bbox.get_center()
                
                # Set a reasonable default viewpoint
                view_control.set_front([0, 0, -1])  # Look at the model from the front
                view_control.set_up([0, -1, 0])     # Up direction
                view_control.set_lookat(center)     # Look at the center of the model
                view_control.set_zoom(zoom)
            
            # Use a light gray background for better visibility
            vis.get_render_option().background_color = np.asarray([0.8, 0.8, 0.8])  # Light gray
            vis.get_render_option().point_size = 3.0  # Larger points
            
            # Debugging camera info
            logger.info(f"Camera configured: zoom={zoom}, looking at center={bbox.get_center()}")
            
            # Render
            vis.poll_events()
            vis.update_renderer()
            image = vis.capture_screen_float_buffer(do_render=True)
            vis.destroy_window()
            
            # Check if the image is all black (or very dark)
            img_array = np.asarray(image)
            if img_array.mean() < 0.1:  # If mean pixel value is very low (almost black)
                logger.warning("Rendered image appears to be all black, creating a fallback")
                # Create a fallback with a message
                fallback_img = np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
                return fallback_img
                
            return img_array
        except Exception as e:
            logger.warning(f"Error rendering point cloud: {str(e)}")
            # Create a fallback image with a light background
            fallback_img = np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
            return fallback_img
    
    def render_mesh_image(self, mesh: o3d.geometry.TriangleMesh, 
                         width: int = 800, height: int = 600,
                         zoom: float = 0.8) -> np.ndarray:
        """
        Render mesh to image without opening a window
        
        Args:
            mesh: Open3D TriangleMesh
            width: Image width
            height: Image height
            zoom: Camera zoom factor
            
        Returns:
            Rendered image as numpy array
        """
        # Debug information
        logger.info(f"Rendering mesh with {len(mesh.triangles)} triangles")
        
        # Check if mesh is valid
        if len(mesh.triangles) == 0:
            logger.warning("Mesh is empty (no triangles), creating a message image instead")
            fallback_img = np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
            return fallback_img
            
        vis = o3d.visualization.Visualizer()
        vis.create_window(visible=False, width=width, height=height)
        vis.add_geometry(mesh)
        
        # Configure camera view
        try:
            view_control = vis.get_view_control()
            if view_control is not None:
                # Position the camera to see the entire mesh
                camera_params = view_control.convert_to_pinhole_camera_parameters()
                
                # Get the bounding box of the mesh
                bbox = mesh.get_axis_aligned_bounding_box()
                center = bbox.get_center()
                
                # Set a reasonable default viewpoint
                view_control.set_front([0, 0, -1])  # Look at the model from the front
                view_control.set_up([0, -1, 0])     # Up direction
                view_control.set_lookat(center)     # Look at the center of the model
                view_control.set_zoom(zoom)
            
            # Use a light gray background for better visibility
            vis.get_render_option().background_color = np.asarray([0.8, 0.8, 0.8])  # Light gray
            vis.get_render_option().mesh_show_back_face = True
            vis.get_render_option().light_on = True
            
            # Debugging camera info
            logger.info(f"Mesh camera configured: zoom={zoom}, looking at center={bbox.get_center()}")
            
            # Render
            vis.poll_events()
            vis.update_renderer()
            image = vis.capture_screen_float_buffer(do_render=True)
            vis.destroy_window()
            
            # Check if the image is all black (or very dark)
            img_array = np.asarray(image)
            if img_array.mean() < 0.1:  # If mean pixel value is very low (almost black)
                logger.warning("Rendered mesh image appears to be all black, creating a fallback")
                
                # Add a white grid pattern to gray background to show something
                fallback_img = np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
                
                # Create mesh texture visualization
                for i in range(0, height, 20):
                    for j in range(0, width, 20):
                        # Create a grid pattern
                        if (i + j) % 40 == 0:
                            fallback_img[i:i+10, j:j+10] = [0.9, 0.9, 0.9]  # Make squares lighter
                
                return fallback_img
                
            return img_array
        except Exception as e:
            logger.warning(f"Error rendering mesh: {str(e)}")
            # Create a fallback image with a light background
            fallback_img = np.ones((height, width, 3), dtype=np.float32) * 0.8  # Light gray
            return fallback_img

    def enhanced_reconstruction(self, depth_map: np.ndarray, image: Image.Image,
                              width: int = 800, height: int = 600,
                              downsample_factor: int = 2) -> Tuple[np.ndarray, o3d.geometry.PointCloud]:
        """
        Create an enhanced 3D reconstruction using depth gradient analysis, confidence-based 
        filtering, and advanced rendering approaches.
        
        Args:
            depth_map: Depth map as numpy array
            image: Original color image
            width: Desired output width
            height: Desired output height
            downsample_factor: Factor by which to downsample the point cloud
            
        Returns:
            Tuple of (rendered image, point cloud)
        """
        logger.info("Creating enhanced 3D reconstruction...")
        
        try:
            # Debug information about the input depth map
            logger.info(f"Depth map shape: {depth_map.shape}, range: [{depth_map.min()}, {depth_map.max()}]")
            
            # Ensure the depth map is valid
            if depth_map.max() <= 0:
                logger.warning("Invalid depth map (all zeros or negative)")
                # Return a fallback colored depth map
                return self.render_depth_map(depth_map, width=width, height=height), o3d.geometry.PointCloud()
            
            # Convert to float for gradient calculation
            depth_float = depth_map.astype(np.float32)
            
            # Calculate depth confidence using gradient analysis
            # Areas with high gradient (edges) are less reliable
            depth_gradx = cv2.Sobel(depth_float, cv2.CV_32F, 1, 0, ksize=3)
            depth_grady = cv2.Sobel(depth_float, cv2.CV_32F, 0, 1, ksize=3)
            depth_grad_mag = np.sqrt(depth_gradx**2 + depth_grady**2)
            
            # Debug gradient info
            logger.info(f"Gradient magnitude range: [{depth_grad_mag.min()}, {depth_grad_mag.max()}]")
            
            # Normalize gradient magnitude
            if depth_grad_mag.max() > 0:
                confidence = 1.0 - (depth_grad_mag / depth_grad_mag.max())
            else:
                confidence = np.ones_like(depth_map)
            
            # Lower the confidence threshold to keep more points while still filtering noise
            confidence_threshold = 0.5  # More forgiving threshold
            confidence_mask = confidence > confidence_threshold
            
            # Count points before and after confidence filtering
            total_points = depth_map.size
            confident_points = np.sum(confidence_mask)
            logger.info(f"Confidence filtering: kept {confident_points}/{total_points} points ({confident_points/total_points*100:.1f}%)")
            
            # Apply confidence mask to depth map
            filtered_depth = depth_map.copy()
            filtered_depth[~confidence_mask] = 0
            
            # Verify filtered depth map has non-zero values
            if np.count_nonzero(filtered_depth) == 0:
                logger.warning("Filtered depth map is empty, using original depth map")
                filtered_depth = depth_map  # Fallback to original
            
            # Create an enhanced colored depth map visualization as a reliable fallback
            enhanced_depth_viz = self.render_depth_map(filtered_depth, width=width, height=height)
            
            # Create a higher quality point cloud with confidence filtering
            pcd = self.depth_to_pointcloud(
                depth_map=filtered_depth,
                image=image,
                downsample_factor=downsample_factor
            )
            
            # Check if point cloud generation succeeded
            if pcd is None or len(pcd.points) == 0:
                logger.warning("Failed to generate point cloud, falling back to colored depth map")
                return enhanced_depth_viz, o3d.geometry.PointCloud()
                
            logger.info(f"Generated point cloud with {len(pcd.points)} points")
            
            # Use a cascading set of rendering approaches, from most advanced to most reliable
            render_img = None
            render_methods = [
                ("Trimesh", lambda: self.render_pointcloud_trimesh(filtered_depth, image, width, height, downsample_factor)),
                ("Plotly", lambda: self.render_pointcloud_plotly(filtered_depth, image, width, height, downsample_factor)), 
                ("Matplotlib", lambda: self.render_pointcloud_matplotlib(filtered_depth, image, width, height, downsample_factor))
            ]
            
            # First try to render directly with the point cloud using the enhanced Trimesh renderer
            try:
                # Try to use Trimesh's point cloud rendering directly with the point cloud
                logger.info("Attempting Trimesh rendering with direct point cloud")
                render_img = self.render_pointcloud_trimesh(
                    point_cloud=pcd,  # Pass the point cloud directly
                    width=width,
                    height=height
                )
                logger.info("Successfully rendered with Trimesh direct point cloud")
                return render_img, pcd
            except Exception as tm_err:
                logger.warning(f"Trimesh direct point cloud rendering failed: {str(tm_err)}")
                
                # Fall back to the tiered approach if direct rendering fails
                render_img = None
                render_methods = [
                    ("Trimesh", lambda: self.render_pointcloud_trimesh(
                        depth_map=filtered_depth, 
                        image=image, 
                        width=width, 
                        height=height, 
                        downsample_factor=downsample_factor
                    )),
                    ("Plotly", lambda: self.render_pointcloud_plotly(
                        filtered_depth, 
                        image, 
                        width=width, 
                        height=height, 
                        downsample_factor=downsample_factor
                    )), 
                    ("Matplotlib", lambda: self.render_pointcloud_matplotlib(
                        filtered_depth, 
                        image, 
                        width=width, 
                        height=height, 
                        downsample_factor=downsample_factor
                    ))
                ]
                
                # Try each rendering method in order until one succeeds
                for method_name, render_func in render_methods:
                    try:
                        logger.info(f"Attempting 3D rendering with {method_name}")
                        render_img = render_func()
                        
                        # Validate the rendered image
                        mean_value = render_img.mean()
                        logger.info(f"{method_name} rendering result: mean value = {mean_value}")
                        
                        # Check if the image is too bright or too dark
                        if mean_value < 0.1 or mean_value > 0.95:
                            logger.warning(f"{method_name} rendering produced invalid image (too bright/dark)")
                            continue
                        
                        logger.info(f"Successfully rendered with {method_name}")
                        break  # Stop if we got a good render
                        
                    except Exception as e:
                        logger.warning(f"{method_name} rendering failed: {str(e)}")
                
                # If all rendering methods failed, use the color depth map
                if render_img is None or render_img.mean() < 0.1 or render_img.mean() > 0.95:
                    logger.warning("All 3D rendering methods failed, using colored depth map")
                    return enhanced_depth_viz, pcd
                
            return render_img, pcd
            
        except Exception as e:
            logger.error(f"Enhanced reconstruction failed: {str(e)}")
            # Return default depth map and empty point cloud as fallback
            render_img = self.render_depth_map(depth_map, width=width, height=height)
            pcd = o3d.geometry.PointCloud()  # Empty point cloud
            return render_img, pcd
            
    def create_error_image(self, width: int = 800, height: int = 600, message: str = "Error processing image") -> np.ndarray:
        """
        Create an error image with text for when visualization fails completely
        
        Args:
            width: Image width
            height: Image height
            message: Error message to display
            
        Returns:
            Error image as numpy array
        """
        # Create a gray background
        img = np.ones((height, width, 3), dtype=np.float32) * 0.8
        
        # Use OpenCV to add text
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        font_thickness = 2
        text_color = (0.2, 0.2, 0.2)  # Dark gray color for text
        
        # Split message by newlines and render each line
        lines = message.split('\n')
        line_height = 30
        y_position = height // 2 - (len(lines) * line_height) // 2
        
        for line in lines:
            # Get the size of the text
            text_size = cv2.getTextSize(line, font, font_scale, font_thickness)[0]
            x_position = (width - text_size[0]) // 2  # Center text horizontally
            
            # Put the text on the image
            cv2.putText(
                img, line, (x_position, y_position), 
                font, font_scale, text_color, font_thickness
            )
            y_position += line_height
            
        return img
    
    def lrm_reconstruction(self, depth_map: np.ndarray, image: Image.Image,
                          width: int = 800, height: int = 600,
                          downsample_factor: int = 2,
                          patch_size: int = 32,
                          overlap: int = 8) -> Tuple[np.ndarray, o3d.geometry.PointCloud]:
        """
        Create a 3D reconstruction using the Local Region Models (LRM) approach.
        This divides the depth map into overlapping patches and processes each 
        independently for more detailed local geometry.
        
        Args:
            depth_map: Depth map as numpy array
            image: Original color image
            width: Desired output width
            height: Desired output height
            downsample_factor: Factor by which to downsample the point cloud
            patch_size: Size of local patches to process
            overlap: Overlap between adjacent patches
            
        Returns:
            Tuple of (rendered image, point cloud)
        """
        logger.info(f"Creating LRM 3D reconstruction (patch_size={patch_size}, overlap={overlap})...")
        
        try:
            # Debug information about the input depth map
            logger.info(f"Depth map shape: {depth_map.shape}, range: [{depth_map.min()}, {depth_map.max()}]")
            
            # Ensure the depth map is valid
            if depth_map.max() <= 0:
                logger.warning("Invalid depth map (all zeros or negative)")
                # Return a fallback colored depth map
                return self.render_depth_map(depth_map, width=width, height=height), o3d.geometry.PointCloud()
            
            # Convert to float for processing
            depth_float = depth_map.astype(np.float32)
            
            # Create an enhanced colored depth map visualization as a reliable fallback
            enhanced_depth_viz = self.render_depth_map(depth_map, width=width, height=height)
            
            # Create empty point cloud to accumulate results from all patches
            combined_pcd = o3d.geometry.PointCloud()
            
            # Get color image as RGB numpy array
            color_img = np.array(image.convert('RGB'))
            
            # Ensure dimensions match
            if color_img.shape[:2] != depth_map.shape[:2]:
                color_img = cv2.resize(color_img, (depth_map.shape[1], depth_map.shape[0]))
            
            # Calculate patch parameters
            h, w = depth_float.shape
            step = patch_size - overlap
            
            # Calculate gradients over the whole image for confidence
            depth_gradx = cv2.Sobel(depth_float, cv2.CV_32F, 1, 0, ksize=3)
            depth_grady = cv2.Sobel(depth_float, cv2.CV_32F, 0, 1, ksize=3)
            depth_grad_mag = np.sqrt(depth_gradx**2 + depth_grady**2)
            
            # Normalize gradient magnitude
            if depth_grad_mag.max() > 0:
                confidence = 1.0 - (depth_grad_mag / depth_grad_mag.max())
            else:
                confidence = np.ones_like(depth_map)
            
            patch_count = 0
            total_patches = ((h - patch_size) // step + 1) * ((w - patch_size) // step + 1)
            logger.info(f"Processing {total_patches} patches...")
            
            # Process each patch
            for y in range(0, h - patch_size + 1, step):
                for x in range(0, w - patch_size + 1, step):
                    # Extract patch
                    depth_patch = depth_float[y:y+patch_size, x:x+patch_size].copy()
                    color_patch = color_img[y:y+patch_size, x:x+patch_size].copy()
                    conf_patch = confidence[y:y+patch_size, x:x+patch_size].copy()
                    
                    # Skip patches with no valid depth
                    if depth_patch.max() <= 0:
                        continue
                    
                    # Apply confidence mask
                    conf_threshold = 0.3  # Lower threshold for local patches
                    conf_mask = conf_patch > conf_threshold
                    filtered_depth = depth_patch.copy()
                    filtered_depth[~conf_mask] = 0
                    
                    # If too many points were filtered out, revert to original
                    if np.count_nonzero(filtered_depth) < 0.3 * np.count_nonzero(depth_patch):
                        filtered_depth = depth_patch
                    
                    # Create local coordinates for the patch
                    patch_y, patch_x = np.mgrid[0:patch_size, 0:patch_size]
                    
                    # Global coordinates (add offset)
                    patch_y += y
                    patch_x += x
                    
                    # Compute 3D coordinates
                    focal_length = 525.0  # Approximate focal length
                    cx, cy = w / 2, h / 2  # Image center
                    
                    # Convert to normalized device coordinates
                    X = (patch_x - cx) * filtered_depth / focal_length
                    Y = (patch_y - cy) * filtered_depth / focal_length
                    Z = filtered_depth
                    
                    # Flatten and combine into points
                    points = np.stack((X.flatten(), Y.flatten(), Z.flatten()), axis=1)
                    colors = color_patch.reshape(-1, 3) / 255.0
                    
                    # Remove invalid points (zero depth)
                    valid_indices = Z.flatten() > 0
                    points = points[valid_indices]
                    colors = colors[valid_indices]
                    
                    # Skip if no valid points
                    if len(points) == 0:
                        continue
                    
                    # Create patch point cloud
                    patch_pcd = o3d.geometry.PointCloud()
                    patch_pcd.points = o3d.utility.Vector3dVector(points)
                    patch_pcd.colors = o3d.utility.Vector3dVector(colors)
                    
                    # Downsample patch point cloud
                    if downsample_factor > 1:
                        patch_pcd = patch_pcd.voxel_down_sample(voxel_size=0.01 * downsample_factor)
                    
                    # Add to combined point cloud
                    combined_pcd += patch_pcd
                    patch_count += 1
                    
                    # Log progress for large images
                    if patch_count % 50 == 0:
                        logger.info(f"Processed {patch_count}/{total_patches} patches...")
            
            logger.info(f"Successfully processed {patch_count} patches with valid depth data")
            
            # Check if point cloud generation succeeded
            if combined_pcd is None or len(combined_pcd.points) == 0:
                logger.warning("Failed to generate point cloud, falling back to colored depth map")
                return enhanced_depth_viz, o3d.geometry.PointCloud()
                
            # Optional statistical outlier removal
            if len(combined_pcd.points) > 100:  # Only if enough points
                logger.info(f"Filtering noise from point cloud with {len(combined_pcd.points)} points")
                combined_pcd, _ = combined_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
            
            logger.info(f"Final point cloud has {len(combined_pcd.points)} points")
            
            # Try to render directly with the generated point cloud using the enhanced Trimesh renderer
            try:
                # Try to use Trimesh's point cloud rendering directly with our combined point cloud
                logger.info("Attempting Trimesh rendering with direct LRM point cloud")
                render_img = self.render_pointcloud_trimesh(
                    point_cloud=combined_pcd,  # Pass the combined point cloud directly
                    width=width,
                    height=height
                )
                logger.info("Successfully rendered LRM result with Trimesh direct point cloud")
                return render_img, combined_pcd
            except Exception as tm_err:
                logger.warning(f"Trimesh direct LRM point cloud rendering failed: {str(tm_err)}")
                
                # Fall back to the tiered approach if direct rendering fails
                try:
                    logger.info("Rendering with Trimesh from depth map")
                    render_img = self.render_pointcloud_trimesh(
                        depth_map=depth_map,
                        image=image,
                        width=width,
                        height=height,
                        downsample_factor=max(1, downsample_factor)
                    )
                except Exception as e:
                    logger.warning(f"Trimesh rendering failed: {str(e)}")
                    try:
                        # Try Plotly next
                        logger.info("Rendering with Plotly")
                        render_img = self.render_pointcloud_plotly(
                            depth_map,
                            image,
                            width=width,
                            height=height,
                            downsample_factor=max(1, downsample_factor)
                        )
                    except Exception as e2:
                        logger.warning(f"Plotly rendering failed: {str(e2)}")
                        # Fall back to Matplotlib as last resort
                        render_img = self.render_pointcloud_matplotlib(
                            depth_map,
                            image,
                            width=width,
                            height=height,
                            downsample_factor=max(1, downsample_factor)
                        )
                
                # Validate the rendered image
                mean_value = render_img.mean()
                if mean_value < 0.1 or mean_value > 0.95:
                    logger.warning(f"LRM rendering produced invalid image (too bright/dark)")
                    return enhanced_depth_viz, combined_pcd
            
            return render_img, combined_pcd
            
        except Exception as e:
            logger.error(f"LRM reconstruction failed: {str(e)}")
            # Return default depth map and empty point cloud as fallback
            render_img = self.render_depth_map(depth_map, width=width, height=height)
            pcd = o3d.geometry.PointCloud()  # Empty point cloud
            return render_img, pcd
    
    def visualize_3d(self, depth_map: np.ndarray, image: Image.Image, 
                    method: str = "depth_map", width: int = 800, height: int = 600) -> np.ndarray:
        """
        Unified method to visualize depth data using various methods with automatic fallbacks
        
        Args:
            depth_map: Depth map as numpy array
            image: Original color image
            method: Visualization method to use (from self.visualization_methods)
            width: Desired output width
            height: Desired output height
            
        Returns:
            Visualization as numpy array (RGB float image)
        """
        logger.info(f"Visualizing 3D with method: {method}")
        
        # First check if the method is valid
        if method not in self.visualization_methods:
            logger.warning(f"Invalid visualization method: {method}, falling back to depth_map")
            method = "depth_map"
            
        # Check if inputs are valid
        if depth_map is None or depth_map.size == 0:
            logger.warning("Invalid depth map provided")
            return self.create_error_image(
                width, height, 
                "Unable to create 3D visualization.\nNo valid depth map available.\nTry a different image."
            )
        
        # Check if image is valid
        if image is None:
            logger.warning("Invalid image provided")
            # Try to use depth map alone if possible
            if method == "depth_map":
                return self.render_depth_map(depth_map, width=width, height=height)
            else:
                # For other methods that need the color image, create a grayscale image from depth
                try:
                    # Create a colored version of depth map to use as texture
                    normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
                    colored = cv2.applyColorMap(normalized, cv2.COLORMAP_INFERNO)
                    colored_rgb = cv2.cvtColor(colored, cv2.COLOR_BGR2RGB)
                    # Convert to PIL image
                    image = Image.fromarray(colored_rgb)
                except Exception as e:
                    logger.warning(f"Failed to create substitute color image: {str(e)}")
                    return self.render_depth_map(depth_map, width=width, height=height)
        
        # Handle errors from previous processing steps (as seen in feedback)
        if hasattr(image, 'size'):
            # Make sure image has the right mode
            if image.mode != 'RGB':
                image = image.convert('RGB')
        else:
            logger.warning("Image object is not a valid PIL image")
            return self.render_depth_map(depth_map, width=width, height=height)
        
        # Process based on method with fallbacks
        try:
            if method == "depth_map":
                # Direct depth map visualization (most reliable)
                return self.render_depth_map(depth_map, width=width, height=height)
                
            elif method == "pointcloud_mpl":
                # Matplotlib point cloud visualization (good fallback)
                return self.render_pointcloud_matplotlib(depth_map, image, 
                                                      width=width, height=height)
                
            elif method == "enhanced_3d":
                # Try to use the enhanced 3D reconstruction 
                try:
                    render_img, _ = self.enhanced_reconstruction(
                        depth_map=depth_map,
                        image=image,
                        width=width,
                        height=height
                    )
                    return render_img
                except RuntimeError as cuda_err:
                    # Handle CUDA-specific errors by falling back to CPU methods
                    if "CUDA" in str(cuda_err):
                        logger.warning(f"CUDA error in enhanced reconstruction: {str(cuda_err)}")
                        logger.info("Falling back to matplotlib visualization (CPU-based)")
                        return self.render_pointcloud_matplotlib(depth_map, image, width=width, height=height)
                    else:
                        # Re-raise non-CUDA runtime errors
                        raise
                        
            elif method == "lrm_3d":
                # Try to use the LRM 3D reconstruction
                try:
                    render_img, _ = self.lrm_reconstruction(
                        depth_map=depth_map,
                        image=image,
                        width=width,
                        height=height,
                        downsample_factor=2,
                        patch_size=32,
                        overlap=8
                    )
                    return render_img
                except RuntimeError as cuda_err:
                    # Handle CUDA-specific errors by falling back to CPU methods
                    if "CUDA" in str(cuda_err):
                        logger.warning(f"CUDA error in LRM reconstruction: {str(cuda_err)}")
                        logger.info("Falling back to matplotlib visualization (CPU-based)")
                        return self.render_pointcloud_matplotlib(depth_map, image, width=width, height=height)
                    else:
                        # Re-raise non-CUDA runtime errors
                        raise
                
            else:
                # Unknown method, fall back to depth map
                logger.warning(f"Unknown visualization method: {method}, falling back to depth_map")
                return self.render_depth_map(depth_map, width=width, height=height)
                
        except Exception as e:
            # If all else fails, fall back to direct depth map visualization
            logger.warning(f"3D visualization failed with error: {str(e)}, falling back to depth map")
            
            # Try depth map rendering which should never fail
            try:
                return self.render_depth_map(depth_map, width=width, height=height)
            except Exception as render_error:
                # If even depth map rendering fails, return error image
                logger.error(f"Depth map rendering also failed: {str(render_error)}")
                return self.create_error_image(
                    width, height, 
                    f"3D visualization failed.\nError: {type(e).__name__}"
                )
